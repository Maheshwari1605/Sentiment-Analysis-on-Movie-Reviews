{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Library Call\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sentiment Class:\n",
    "    0 - Negative\n",
    "    1 - Somewhat Negative\n",
    "    2 - Neutral\n",
    "    3 - Somewhat Positive\n",
    "    4 - Positive\n",
    "    \\t - Tab\n",
    "\"\"\"\n",
    "train_df = pd.read_csv(\"../Data/train.tsv\",sep = \"\\t\")\n",
    "test_df = pd.read_csv(\"../Data/test.tsv\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (train_df.shape)\n",
    "# print (train_df.head())\n",
    "class_distribution = train_df[\"Sentiment\"].value_counts()\n",
    "print (class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(np.array(class_distribution.index),np.array(class_distribution.values))\n",
    "plt.xlabel(\"Sentiment Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8529\n",
      "18.297572986282095\n"
     ]
    }
   ],
   "source": [
    "u_num_reviews = len(train_df[\"SentenceId\"].unique())\n",
    "avg_word_per_sentence  = train_df.groupby(\"SentenceId\")[\"Phrase\"].count().mean()\n",
    "\n",
    "print (u_num_reviews)\n",
    "print (avg_word_per_sentence)\n",
    "\n",
    "# print (train_df[\"Phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud Visualization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stopwords = set(list(STOPWORDS)+ list(stopwords.words('english')))\n",
    "\n",
    "\n",
    "def wcloud(phrase):\n",
    "    wordcloud = WordCloud(background_color='black',stopwords=stopwords,random_state=1).generate(str(phrase))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcloud(train_df[train_df[\"Sentiment\"]==4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Phrase\"] = train_df[\"Phrase\"].str.lower()  #Normalizing the words- Everything to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "w_tokenizer = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemma(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w  in w_tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Processed_Phrase\"] = train_df.Phrase.apply(lemma)\n",
    "test_df[\"Processed_Phrase\"] = test_df.Phrase.apply(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         2\n",
      "2         2\n",
      "3         2\n",
      "4         2\n",
      "5         2\n",
      "6         2\n",
      "7         2\n",
      "8         2\n",
      "9         2\n",
      "10        2\n",
      "11        2\n",
      "12        2\n",
      "13        2\n",
      "14        2\n",
      "15        2\n",
      "16        2\n",
      "17        2\n",
      "18        2\n",
      "19        2\n",
      "20        2\n",
      "21        3\n",
      "22        3\n",
      "23        2\n",
      "24        2\n",
      "25        2\n",
      "26        2\n",
      "27        2\n",
      "28        2\n",
      "29        2\n",
      "         ..\n",
      "156030    2\n",
      "156031    1\n",
      "156032    1\n",
      "156033    1\n",
      "156034    1\n",
      "156035    2\n",
      "156036    1\n",
      "156037    2\n",
      "156038    2\n",
      "156039    2\n",
      "156040    2\n",
      "156041    2\n",
      "156042    2\n",
      "156043    3\n",
      "156044    2\n",
      "156045    2\n",
      "156046    2\n",
      "156047    1\n",
      "156048    2\n",
      "156049    2\n",
      "156050    2\n",
      "156051    1\n",
      "156052    1\n",
      "156053    2\n",
      "156054    2\n",
      "156055    2\n",
      "156056    1\n",
      "156057    3\n",
      "156058    2\n",
      "156059    2\n",
      "Name: Sentiment, Length: 156060, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# from nltk.tokenize import TweetTokenizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "# full_text = list(train_df['Processed_Phrase'].values) + list(test_df['Processed_Phrase'].values)\n",
    "# vectorizer.fit(full_text)\n",
    "# train_vectorized = vectorizer.transform(train_df['Processed_Phrase']) #TFIDF vector for train data set\n",
    "\n",
    "Y = train_df[\"Sentiment\"] #Labels\n",
    "\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# target = train_df.Sentiment.values\n",
    "Y = to_categorical(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 13000\n",
    "max_words = 50\n",
    "batch_size = 128\n",
    "epochs = 7\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(train_df[\"Processed_Phrase\"],Y,test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,GRU,LSTM,Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import SpatialDropout1D,Dropout,Bidirectional,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Flatten\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vectorizer_new_v2', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318425, 50)\n",
      "(318425, 5)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "smote = SMOTE(random_state=0)\n",
    "train_df_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "x_train = train_df_smote\n",
    "y_train = y_smote\n",
    "\n",
    "y_train_org = np.argmax(y_train, axis=-1)\n",
    "# y_dummy_df = pd.DataFrame({\"Balanced_Labels\":y_train_org})\n",
    "# print (y_dummy_df[\"Balanced_Labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.15      0.03       141\n",
      "           1       0.00      0.50      0.00         6\n",
      "           2       0.00      0.50      0.00         2\n",
      "           3       0.00      0.33      0.00        15\n",
      "           4       0.00      0.12      0.01        42\n",
      "\n",
      "   micro avg       0.00      0.17      0.00       206\n",
      "   macro avg       0.00      0.32      0.01       206\n",
      "weighted avg       0.01      0.17      0.02       206\n",
      " samples avg       0.00      0.00      0.00       206\n",
      "\n",
      "0.0009611687812379854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lin_reg = LogisticRegression()\n",
    "one_vs_all_regression = OneVsRestClassifier(lin_reg)\n",
    "one_vs_all_regression.fit(x_train,y_train)\n",
    "print(classification_report( one_vs_all_regression.predict(x_test) , y_test))\n",
    "print(accuracy_score( one_vs_all_regression.predict(x_test) , y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=5,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree - Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=5,n_estimators=5,criterion=\"entropy\")\n",
    "rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.25      0.25      1475\n",
      "           1       0.28      0.37      0.32      4211\n",
      "           2       0.41      0.68      0.51      9540\n",
      "           3       0.29      0.39      0.33      4899\n",
      "           4       0.27      0.26      0.26      1876\n",
      "\n",
      "   micro avg       0.35      0.49      0.41     22001\n",
      "   macro avg       0.30      0.39      0.34     22001\n",
      "weighted avg       0.34      0.49      0.40     22001\n",
      " samples avg       0.35      0.35      0.35     22001\n",
      "\n",
      "0.3469178521081635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( rf.predict(x_test) , y_test))\n",
    "print(accuracy_score( rf.predict(x_test) , y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1300000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 64)          31680     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                9312      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,341,157\n",
      "Trainable params: 1,341,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_GRU=Sequential()\n",
    "model_GRU.add(Embedding(max_features,100,mask_zero=True))\n",
    "model_GRU.add(GRU(64,dropout=0.4,return_sequences=True))\n",
    "model_GRU.add(GRU(32,dropout=0.5,return_sequences=False))\n",
    "model_GRU.add(Dense(num_classes,activation='softmax'))\n",
    "model_GRU.compile(loss='categorical_crossentropy',optimizer=Adam(lr = 0.001),metrics=['categorical_accuracy'])\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 318425 samples, validate on 31212 samples\n",
      "Epoch 1/2\n",
      "318425/318425 [==============================] - 942s 3ms/step - loss: 1.3910 - categorical_accuracy: 0.4080 - val_loss: 1.1015 - val_categorical_accuracy: 0.5902\n",
      "Epoch 2/2\n",
      "318425/318425 [==============================] - 1180s 4ms/step - loss: 1.2331 - categorical_accuracy: 0.4850 - val_loss: 1.0648 - val_categorical_accuracy: 0.6020\n"
     ]
    }
   ],
   "source": [
    "history1=model_GRU.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31212/31212 [==============================] - 71s 2ms/step\n",
      "[3 1 4 ... 4 2 1]\n",
      "[2 2 3 ... 2 1 2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.35      0.39      1771\n",
      "           1       0.50      0.49      0.50      5585\n",
      "           2       0.71      0.75      0.73     15252\n",
      "           3       0.45      0.54      0.49      5571\n",
      "           4       0.60      0.36      0.45      3033\n",
      "\n",
      "    accuracy                           0.60     31212\n",
      "   macro avg       0.54      0.50      0.51     31212\n",
      "weighted avg       0.60      0.60      0.60     31212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test_pred=model_GRU.predict_classes(x_test, verbose=1)\n",
    "\n",
    "y_test_num = np.argmax(y_test, axis=-1)\n",
    "\n",
    "print (y_test_pred)\n",
    "print (y_test_num)\n",
    "\n",
    "print(classification_report( y_test_pred, y_test_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============================================================LSTM=========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1300000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, None, 64)          42240     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,354,821\n",
      "Trainable params: 1,354,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3_LSTM=Sequential()\n",
    "model3_LSTM.add(Embedding(max_features,100,mask_zero=True))\n",
    "model3_LSTM.add(LSTM(64,dropout=0.4,return_sequences=True))\n",
    "model3_LSTM.add(LSTM(32,dropout=0.5,return_sequences=False))\n",
    "model3_LSTM.add(Dense(num_classes,activation='sigmoid'))\n",
    "model3_LSTM.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=['categorical_accuracy'])\n",
    "model3_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 318425 samples, validate on 31212 samples\n",
      "Epoch 1/7\n",
      "318425/318425 [==============================] - 871s 3ms/step - loss: 0.4642 - categorical_accuracy: 0.3634 - val_loss: 0.3748 - val_categorical_accuracy: 0.5858\n",
      "Epoch 2/7\n",
      "318425/318425 [==============================] - 1262s 4ms/step - loss: 0.4128 - categorical_accuracy: 0.4660 - val_loss: 0.3684 - val_categorical_accuracy: 0.5846\n",
      "Epoch 3/7\n",
      "310144/318425 [============================>.] - ETA: 17s - loss: 0.3916 - categorical_accuracy: 0.4988"
     ]
    }
   ],
   "source": [
    "history3=model3_LSTM.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test_pred=model3_LSTM.predict_classes(x_test, verbose=1)\n",
    "\n",
    "y_test_num = np.argmax(y_test, axis=-1)\n",
    "\n",
    "dum_df = pd.DataFrame({\"Y_Test\":y_test_num})\n",
    "print (dum_df[\"Y_Test\"].value_counts())\n",
    "\n",
    "# print (y_test_pred)\n",
    "# print (y_test_num)\n",
    "\n",
    "print(classification_report( y_test_pred, y_test_num))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lstm_model', 'wb') as output:\n",
    "    pickle.dump(model3_LSTM, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
